//TwitterLDA with background topic
package hoang.l3s.attt.evaluation;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileReader;
import java.io.FileWriter;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Calendar;
import java.util.Date;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Random;

import hoang.l3s.attt.configure.Configure;
import hoang.l3s.attt.utils.RankingUtils;
import hoang.l3s.attt.utils.SimilarityUtils;
import hoang.l3s.attt.utils.TweetPreprocessingUtils;
import weka.filters.unsupervised.attribute.AddValues;

public class TwitterLDA {
	private long startTime = 1485554401000L;
	private long TIME_STEP_WIDTH = 30 * 60 * 1000;// 30 mins
	//
	private String dataPath;
	private String outputPath;
	private HashMap<String, String> allRawTweets;

	private int nTopics;

	// data-preporocessing
	private int Term_Min_NTweets = 5;
	private int Tweet_Min_NTerms = 3;
	private int Epoch_Min_NTweets = 10;

	private int burningPeriod;
	private int maxIteration;
	private int samplingGap;

	private Random rand;
	// hyperparameters
	private double alpha;
	private double sum_alpha;
	private double beta;
	private double sum_beta;
	private double[] gamma;
	private double sum_gamma;
	// data
	private Epoch[] epoches;

	private String[] tweetVocabulary;
	// parameters
	private double[][] tweetTopics;
	private double[] backgroundTopic;
	private double[] coinBias;

	// Gibbs sampling variables
	// epoch - topic count
	private int[][] n_ze; // n_zu[k,u]: number of times topic z is observed in
							// tweets by epoch e
	private int[] sum_nze;// sum_nzu[u]: total number of topics that are
							// observed in tweets by epoch e
	// topic - word count
	private int[][] n_wz;// n_wz[w,z]: number of times word w is generated by a
							// topic z in all tweets
	private int[] sum_nwz;// sum_nw[z]: total number of words that are generated
							// by a topic z in tweets
	private int[] n_wb;// n_wz[w]: number of times word w is generated by a
						// background topic
	private int sum_nwb;// sum_nw[z]: total number of words that are generated
						// by background topic

	// topic - coin count
	private int[] n_c;// sum_nw[c]: total number of words that are
						// associated with coin c
	private int sum_nc;

	private int[][] final_n_ze;
	private int[] final_sum_nze;
	private int[][] final_n_wz;
	private int[] final_sum_nwz;
	private int[] final_n_wb;
	private int final_sum_nwb;
	private int[] final_n_c;
	private int final_sum_nc;

	private double tweetLogLikelidhood;

	private HashMap<String, Integer> tweetId2EpochIndex;
	private HashMap<String, Integer> tweetId2TweetIndex;
	private HashMap<Integer, HashSet<String>> topicTopTweets;

	public TwitterLDA(String _dataPath, String _outputPath, int _nTopics) {
		dataPath = _dataPath;
		outputPath = _outputPath;
		nTopics = _nTopics;

		burningPeriod = 10;
		maxIteration = 50;
		samplingGap = 2;

	}

	private int getEpoch(long time) {
		int t = (int) ((time - startTime) / TIME_STEP_WIDTH);
		if (t < 0)
			t = 0;
		return t;
	}

	public void readData() {
		try {

			allRawTweets = new HashMap<String, String>();
			TweetPreprocessingUtils preprocessingUtils = new TweetPreprocessingUtils();
			BufferedReader br = new BufferedReader(new FileReader(dataPath));
			HashMap<String, Integer> wordNTweets = new HashMap<String, Integer>();
			HashMap<Integer, HashMap<String, List<String>>> rawEpoches = new HashMap<Integer, HashMap<String, List<String>>>();

			String line = null;
			int nTweets = 0;
			while ((line = br.readLine()) != null) {
				String[] tokens = line.split("\t");
				int epochId = getEpoch(Long.parseLong(tokens[2]));
				String tweetId = tokens[0];
				allRawTweets.put(tweetId, tokens[4]);
				List<String> terms = preprocessingUtils.extractTermInTweet(tokens[4]);
				for (int i = 0; i < terms.size(); i++) {
					String word = terms.get(i);
					if (wordNTweets.containsKey(word)) {
						wordNTweets.put(word, 1 + wordNTweets.get(word));
					} else {
						wordNTweets.put(word, 1);
					}
				}
				HashMap<String, List<String>> tweets = rawEpoches.get(epochId);
				if (tweets != null) {
					tweets.put(tweetId, terms);
				} else {
					tweets = new HashMap<String, List<String>>();
					tweets.put(tweetId, terms);
					rawEpoches.put(epochId, tweets);
				}
				nTweets++;
			}
			br.close();

			// filter less frequent words and short tweets

			HashSet<String> removedTerms = new HashSet<String>();
			HashSet<String> removedTweets = new HashSet<String>();
			HashSet<Integer> removedEpoches = new HashSet<Integer>();
			while (true) {
				boolean flag = true;
				for (Map.Entry<Integer, HashMap<String, List<String>>> epoch : rawEpoches.entrySet()) {
					int e = epoch.getKey();
					if (removedEpoches.contains(e)) {
						continue;
					}
					nTweets = 0;
					HashMap<String, List<String>> tweets = epoch.getValue();
					for (Map.Entry<String, List<String>> tweet : tweets.entrySet()) {
						String tId = tweet.getKey();
						if (removedTweets.contains(tId)) {
							continue;
						}
						List<String> terms = tweet.getValue();
						int nTerms = 0;
						for (int i = 0; i < terms.size(); i++) {
							String word = terms.get(i);
							if (removedTerms.contains(word)) {
								continue;
							}
							if (wordNTweets.get(word) >= Term_Min_NTweets) {
								nTerms++;
							} else {
								removedTerms.add(word);
								flag = false;
							}
						}
						if (nTerms < Tweet_Min_NTerms) {// reduce tweet counts
														// of terms in this
														// tweet
							for (int i = 0; i < terms.size(); i++) {
								String word = terms.get(i);
								wordNTweets.put(word, wordNTweets.get(word) - 1);
							}
							removedTweets.add(tId);
							flag = false;
						} else {
							nTweets++;
						}
					}
					if (nTweets < Epoch_Min_NTweets) {// remove all tweets in
														// this epoch
						for (Map.Entry<String, List<String>> tweet : tweets.entrySet()) {
							String tId = tweet.getKey();
							if (removedTweets.contains(tId)) {
								continue;
							}
							List<String> terms = tweet.getValue();
							for (int i = 0; i < terms.size(); i++) {
								String word = terms.get(i);
								if (removedTerms.contains(word)) {
									continue;
								}
								wordNTweets.put(word, wordNTweets.get(word) - 1);
							}
						}
						removedEpoches.add(e);
						flag = false;
					}
				}
				if (flag)
					break;
			}

			HashMap<String, Integer> word2Index = new HashMap<String, Integer>();

			epoches = new Epoch[rawEpoches.size() - removedEpoches.size()];
			int epochId = 0;
			for (Map.Entry<Integer, HashMap<String, List<String>>> epoch : rawEpoches.entrySet()) {
				int e = epoch.getKey();
				if (removedEpoches.contains(e)) {
					continue;
				}
				nTweets = 0;
				HashMap<String, List<String>> allTweets = epoch.getValue();
				for (Map.Entry<String, List<String>> tweet : allTweets.entrySet()) {
					String tId = tweet.getKey();
					if (removedTweets.contains(tId)) {
						continue;
					}
					nTweets++;
				}
				Tweet[] tweets = new Tweet[nTweets];
				nTweets = 0;
				for (Map.Entry<String, List<String>> tweet : allTweets.entrySet()) {
					String tId = tweet.getKey();
					if (removedTweets.contains(tId)) {
						continue;
					}
					List<String> terms = tweet.getValue();
					int nTerms = 0;
					for (int i = 0; i < terms.size(); i++) {
						String word = terms.get(i);
						if (removedTerms.contains(word)) {
							continue;
						}
						nTerms++;
					}
					int[] words = new int[nTerms];
					nTerms = 0;
					for (int i = 0; i < terms.size(); i++) {
						String word = terms.get(i);
						if (removedTerms.contains(word)) {
							continue;
						}
						if (word2Index.containsKey(word)) {
							words[nTerms] = word2Index.get(word);
						} else {
							words[nTerms] = word2Index.size();
							word2Index.put(word, word2Index.size());
						}
						nTerms++;
					}
					tweets[nTweets] = new Tweet();
					tweets[nTweets].tweetID = tId;
					tweets[nTweets].words = words;
					nTweets++;
				}
				epoches[epochId] = new Epoch();
				epoches[epochId].epochId = e;
				epoches[epochId].tweets = tweets;
				epochId++;
			}

			tweetVocabulary = new String[word2Index.size()];
			for (Map.Entry<String, Integer> word : word2Index.entrySet()) {
				tweetVocabulary[word.getValue()] = word.getKey();
			}

		} catch (Exception e) {
			e.printStackTrace();
			System.exit(-1);
		}
	}

	public void outputReformattedData() {
		try {
			BufferedWriter bw_stats = new BufferedWriter(
					new FileWriter(String.format("%s/twiterLDA_stats.csv", outputPath)));
			BufferedWriter bw_tweets = new BufferedWriter(
					new FileWriter(String.format("%s/twiterLDA_tweets.csv", outputPath)));
			for (int i = 0; i < epoches.length; i++) {
				Epoch e = epoches[i];
				bw_stats.write(String.format("%d,%d\n", e.epochId, e.tweets.length));
				for (int j = 0; j < e.tweets.length; j++) {
					bw_tweets.write(String.format("%d\t%s", i, e.tweets[j].tweetID));
					for (int w = 0; w < e.tweets[j].words.length; w++) {
						bw_tweets.write(String.format(" %s", tweetVocabulary[e.tweets[j].words[w]]));
					}
					bw_tweets.write("\n");
				}
			}
			bw_stats.close();
			bw_tweets.close();

			bw_stats = new BufferedWriter(new FileWriter(String.format("%s/twiterLDA_vocab.csv", outputPath)));
			for (int i = 0; i < tweetVocabulary.length; i++) {
				bw_stats.write(String.format("%d,%s\n", i, tweetVocabulary[i]));
			}
			bw_stats.close();

		} catch (Exception e) {
			e.printStackTrace();
		}
	}

	private void declareFinalCounts() {
		final_n_ze = new int[nTopics][epoches.length];
		final_sum_nze = new int[epoches.length];
		for (int e = 0; e < epoches.length; e++) {
			for (int z = 0; z < nTopics; z++)
				final_n_ze[z][e] = 0;
			final_sum_nze[e] = 0;
		}
		final_n_wz = new int[tweetVocabulary.length][nTopics];
		final_sum_nwz = new int[nTopics];
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				final_n_wz[w][z] = 0;
			final_sum_nwz[z] = 0;
		}

		final_n_wb = new int[tweetVocabulary.length];
		for (int w = 0; w < tweetVocabulary.length; w++)
			final_n_wb[w] = 0;
		final_sum_nwb = 0;

		final_n_c = new int[2];
		for (int c = 0; c < 2; c++) {
			final_n_c[c] = 0;
		}
		final_sum_nc = 0;
	}

	private void initilize() {
		rand = new Random();

		// init coin and topic for each tweet and each behavior
		for (int e = 0; e < epoches.length; e++) {
			// tweet
			for (int t = 0; t < epoches[e].tweets.length; t++) {
				epoches[e].tweets[t].topic = rand.nextInt(nTopics);
				int nWords = epoches[e].tweets[t].words.length;
				epoches[e].tweets[t].coins = new int[nWords];
				for (int i = 0; i < epoches[e].tweets[t].coins.length; i++)
					epoches[e].tweets[t].coins[i] = rand.nextInt(2);
			}
		}
		// declare and initiate counting tables
		n_ze = new int[nTopics][epoches.length];
		sum_nze = new int[epoches.length];
		for (int e = 0; e < epoches.length; e++) {
			for (int z = 0; z < nTopics; z++)
				n_ze[z][e] = 0;
			sum_nze[e] = 0;
		}
		n_wz = new int[tweetVocabulary.length][nTopics];
		sum_nwz = new int[nTopics];
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				n_wz[w][z] = 0;
			sum_nwz[z] = 0;
		}
		n_wb = new int[tweetVocabulary.length];
		for (int w = 0; w < tweetVocabulary.length; w++)
			n_wb[w] = 0;
		sum_nwb = 0;

		n_c = new int[2];
		n_c[0] = 0;
		n_c[1] = 0;
		sum_nc = 0;
		// update counting tables
		for (int e = 0; e < epoches.length; e++) {
			// tweet
			for (int t = 0; t < epoches[e].tweets.length; t++) {
				int z = epoches[e].tweets[t].topic;
				// user-topic and community-topic
				n_ze[z][e]++;
				sum_nze[e]++;
				for (int i = 0; i < epoches[e].tweets[t].words.length; i++) {
					int w = epoches[e].tweets[t].words[i];
					int c = epoches[e].tweets[t].coins[i];
					// coin count
					n_c[c]++;
					sum_nc++;
					if (c == 0) {
						// word - background topic
						n_wb[w]++;
						sum_nwb++;
					} else {
						// word - topic
						n_wz[w][z]++;
						sum_nwz[z]++;
					}

				}
			}
		}
	}

	// sampling
	private void setPriors() {
		// user topic prior
		alpha = 50.0 / nTopics;
		sum_alpha = 50;

		// topic tweet word prior
		beta = 0.01;
		sum_beta = 0.01 * tweetVocabulary.length;
		// biased coin prior
		gamma = new double[2];
		gamma[0] = 2;
		gamma[1] = 2;
		sum_gamma = gamma[0] + gamma[1];
	}

	private void sampleTweetTopic(int u, int t) {
		// sample the topic for tweet number t of user number u
		// get current topic
		int currz = epoches[u].tweets[t].topic;
		// sampling based on user interest
		n_ze[currz][u]--;
		sum_nze[u]--;
		for (int i = 0; i < epoches[u].tweets[t].words.length; i++) {
			if (epoches[u].tweets[t].coins[i] == 0)
				continue;// do not consider background words
			int w = epoches[u].tweets[t].words[i];
			n_wz[w][currz]--;
			sum_nwz[currz]--;
		}
		double sump = 0;
		double[] p = new double[nTopics];
		for (int z = 0; z < nTopics; z++) {
			p[z] = (n_ze[z][u] + alpha) / (sum_nze[u] + sum_alpha);
			for (int i = 0; i < epoches[u].tweets[t].words.length; i++) {
				if (epoches[u].tweets[t].coins[i] == 0)
					continue;// do not consider background words
				int w = epoches[u].tweets[t].words[i];
				p[z] = p[z] * (n_wz[w][z] + beta) / (sum_nwz[z] + sum_beta);
			}
			// cumulative
			p[z] = sump + p[z];
			sump = p[z];
		}
		sump = rand.nextDouble() * sump;
		for (int z = 0; z < nTopics; z++) {
			if (sump > p[z])
				continue;
			// the topic
			epoches[u].tweets[t].topic = z;
			// user - topic
			n_ze[z][u]++;
			sum_nze[u]++;
			// topic - word
			for (int i = 0; i < epoches[u].tweets[t].words.length; i++) {
				if (epoches[u].tweets[t].coins[i] == 0)
					continue;// do not consider background words
				int w = epoches[u].tweets[t].words[i];
				n_wz[w][z]++;
				sum_nwz[z]++;
			}
			return;
		}
		System.out.println("bug in sampleTweetTopic");
		for (int z = 0; z < nTopics; z++) {
			System.out.print(p[z] + " ");
		}
		System.exit(-1);
	}

	private void sampleWordCoin(int e, int t, int i) {
		// sample the coin for the word number i of the tweet number t of epoch
		// number e
		// get current coin
		int currc = epoches[e].tweets[t].coins[i];
		// get current word
		int w = epoches[e].tweets[t].words[i];
		// get current topic
		int z = epoches[e].tweets[t].topic;
		// coin count
		n_c[currc]--;
		sum_nc--;
		if (currc == 0) {
			// word - background topic
			n_wb[w]--;
			sum_nwb--;
		} else {
			// word - topic
			n_wz[w][z]--;
			sum_nwz[z]--;
		}

		// probability of coin 0 given priors and recent counts
		double p_0 = (n_c[0] + gamma[0]) / (sum_nc + sum_gamma);
		// probability of w given coin 0
		p_0 = p_0 * (n_wb[w] + beta) / (sum_nwb + sum_beta);

		// probability of coin 1 given priors and recent counts
		double p_1 = (n_c[1] + gamma[1]) / (sum_nc + sum_gamma);
		// probability of w given coin 1 and topic z
		p_1 = p_1 * (n_wz[w][z] + beta) / (sum_nwz[z] + sum_beta);

		double sump = p_0 + p_1;
		sump = rand.nextDouble() * sump;
		int c = 0;
		if (sump > p_0)
			c = 1;
		// the coin
		epoches[e].tweets[t].coins[i] = c;
		// coin count
		n_c[c]++;
		sum_nc++;
		if (c == 0) {
			// word-background topic
			n_wb[w]++;
			sum_nwb++;
		} else {
			// word - topic
			n_wz[w][z]++;
			sum_nwz[z]++;
		}
	}

	private void updateFinalCounts() {
		for (int e = 0; e < epoches.length; e++) {
			for (int z = 0; z < nTopics; z++)
				final_n_ze[z][e] += n_ze[z][e];
			final_sum_nze[e] += sum_nze[e];
		}

		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				final_n_wz[w][z] += n_wz[w][z];
			final_sum_nwz[z] += sum_nwz[z];
		}

		for (int w = 0; w < tweetVocabulary.length; w++)
			final_n_wb[w] += n_wb[w];
		final_sum_nwb += sum_nwb;

		for (int c = 0; c < 2; c++) {
			final_n_c[c] += n_c[c];
		}
		final_sum_nc += sum_nc;
	}

	private void gibbsSampling() {
		System.out.println("Runing Gibbs sampling");
		System.out.print("Setting prios ...");
		setPriors();
		System.out.println(" Done!");
		declareFinalCounts();
		System.out.print("Initializing ... ");
		initilize();
		System.out.println("... Done!");
		for (int iter = 0; iter < burningPeriod + maxIteration; iter++) {
			System.out.print("iteration " + iter);
			// topic
			for (int e = 0; e < epoches.length; e++) {
				for (int t = 0; t < epoches[e].tweets.length; t++) {
					sampleTweetTopic(e, t);
				}
			}
			// coin
			for (int e = 0; e < epoches.length; e++) {
				for (int t = 0; t < epoches[e].tweets.length; t++) {
					for (int i = 0; i < epoches[e].tweets[t].words.length; i++)
						sampleWordCoin(e, t, i);
				}
			}

			System.out.println(" done!");
			if (samplingGap <= 0)
				continue;
			if (iter < burningPeriod)
				continue;
			if ((iter - burningPeriod) % samplingGap == 0) {
				updateFinalCounts();
			}
		}
		if (samplingGap <= 0)
			updateFinalCounts();
	}

	// inference
	private void inferingModelParameters() {
		// epoch
		for (int e = 0; e < epoches.length; e++) {
			// topic distribution
			epoches[e].topicDistribution = new double[nTopics];
			for (int z = 0; z < nTopics; z++) {
				epoches[e].topicDistribution[z] = (final_n_ze[z][e] + alpha) / (final_sum_nze[e] + sum_alpha);
			}
		}
		// topics
		tweetTopics = new double[nTopics][tweetVocabulary.length];
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				tweetTopics[z][w] = (final_n_wz[w][z] + beta) / (final_sum_nwz[z] + sum_beta);
		}

		// background topics
		backgroundTopic = new double[tweetVocabulary.length];
		for (int w = 0; w < tweetVocabulary.length; w++)
			backgroundTopic[w] = (final_n_wb[w] + beta) / (final_sum_nwb + sum_beta);
		// coin bias
		coinBias = new double[2];
		coinBias[0] = (final_n_c[0] + gamma[0]) / (final_sum_nc + sum_gamma);
		coinBias[1] = (final_n_c[1] + gamma[1]) / (final_sum_nc + sum_gamma);

	}

	private double getTweetLikelihood(int u, int t) {
		// compute likelihood of tweet number t of user number u
		double logLikelihood = 0;
		for (int i = 0; i < epoches[u].tweets[t].words.length; i++) {
			int w = epoches[u].tweets[t].words[i];
			// probability that word i is generated by background topic
			double p_0 = backgroundTopic[w] * coinBias[0];
			// probability that word i is generated by other topics
			double p_1 = 0;
			for (int z = 0; z < nTopics; z++) {
				double p_z = tweetTopics[z][w] * epoches[u].topicDistribution[z];
				p_1 = p_1 + p_z;
			}
			p_1 = p_1 * coinBias[1];

			logLikelihood = logLikelihood + Math.log10(p_0 + p_1);
			/*
			 * if (Double.isNaN(logLikelihood)) { System.out.println("p_0 = " +
			 * p_0 + "\tp_1 = " + p_1); // System.exit(-1); }
			 */
		}

		return logLikelihood;
	}

	private double getTweetLikelihood(int e, int t, int z) {
		// compute likelihood of tweet number t of epoche e given the topic
		// z
		if (z >= 0) {
			double logLikelihood = 0;
			for (int i = 0; i < epoches[e].tweets[t].words.length; i++) {
				int w = epoches[e].tweets[t].words[i];
				// probability that word i is generated by background topic
				double p_0 = backgroundTopic[w] * coinBias[0];
				// probability that word i is generated by topic z
				double p_1 = tweetTopics[z][w] * coinBias[1];
				logLikelihood = logLikelihood + Math.log10(p_0 + p_1);
			}
			return logLikelihood;
		} else {
			double logLikelihood = 0;
			for (int i = 0; i < epoches[e].tweets[t].words.length; i++) {
				int w = epoches[e].tweets[t].words[i];
				// probability that word i is generated by background topic
				double p_0 = backgroundTopic[w];
				logLikelihood = logLikelihood + Math.log10(p_0);
			}
			return logLikelihood;
		}
	}

	private double getTweetSumProbUniqueWord(int e, int t, int z) {
		// compute likelihood of tweet number t of epoche e given the topic
		// z

		HashSet<Integer> uniqueWords = new HashSet<Integer>();
		for (int i = 0; i < epoches[e].tweets[t].words.length; i++) {
			int w = epoches[e].tweets[t].words[i];
			uniqueWords.add(w);
		}

		double sum = 0;
		if (z >= 0) {
			for (int w : uniqueWords) {
				// probability that word i is generated by background topic
				double p_0 = backgroundTopic[w] * coinBias[0];
				// probability that word i is generated by topic z
				double p_1 = tweetTopics[z][w] * coinBias[1];
				sum += p_0 + p_1;

			}
		} else {
			for (int w : uniqueWords) {
				// probability that word i is generated by background topic
				double p_0 = backgroundTopic[w];
				sum += p_0;
			}
		}
		return sum;
	}

	private double getTweetEntropy(int e, int t) {
		double[] prob = new double[nTopics];
		double sum = 0;
		for (int z = 0; z < nTopics; z++) {
			prob[z] = getTweetLikelihood(e, t, z);
			prob[z] = Math.exp(prob[z]);
			sum += prob[z];
		}
		double entropy = 0;
		for (int z = 0; z < nTopics; z++) {
			prob[z] /= sum;
			entropy += prob[z] * Math.log(prob[z]);
		}
		return entropy;
	}

	private void getLikelihoodPerplexity() {
		tweetLogLikelidhood = 0;
		for (int e = 0; e < epoches.length; e++) {
			for (int t = 0; t < epoches[e].tweets.length; t++) {
				double logLikelihood = getTweetLikelihood(e, t);
				tweetLogLikelidhood += logLikelihood;
			}
		}
	}

	private void inferTweetTopic() {
		for (int e = 0; e < epoches.length; e++) {
			for (int t = 0; t < epoches[e].tweets.length; t++) {
				epoches[e].tweets[t].inferedTopic = -1;// background topic only
				epoches[e].tweets[t].inferedLikelihood = epoches[e].tweets.length * Math.log10(coinBias[0])
						+ getTweetLikelihood(e, t, -1);

				for (int z = 0; z < nTopics; z++) {
					double p_z = getTweetLikelihood(e, t, z);
					p_z += Math.log10(epoches[e].topicDistribution[z]);

					if (epoches[e].tweets[t].inferedLikelihood < p_z) {
						epoches[e].tweets[t].inferedLikelihood = p_z;
						epoches[e].tweets[t].inferedTopic = z;
					}
				}
			}
		}
	}

	private void outputTweetTopics() {
		try {
			String fileName = outputPath + "/tweetTopics.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			for (int z = 0; z < nTopics; z++) {
				bw.write("" + z);
				for (int w = 0; w < tweetVocabulary.length; w++)
					bw.write("," + tweetTopics[z][w]);
				bw.write("\n");
			}
			bw.write("background");
			for (int w = 0; w < tweetVocabulary.length; w++)
				bw.write("," + backgroundTopic[w]);
			bw.write("\n");
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out tweet topics to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputCoinBias() {
		try {
			String fileName = outputPath + "/coinBias.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			bw.write(coinBias[0] + "," + coinBias[1]);
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out coin bias to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputTweetTopicTopWords(int k) {
		try {
			String fileName = outputPath + "/tweetTopicTopWords.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			List<Integer> topWords = null;
			for (int z = 0; z < nTopics; z++) {
				bw.write(String.format("***[[TOPIC-%d]]***\n", z));
				topWords = RankingUtils.getIndexTopElements(k, tweetTopics[z]);
				for (int j = topWords.size() - 1; j >= 0; j--) {
					int w = topWords.get(j);
					bw.write(String.format("%s,%f\n", tweetVocabulary[w], tweetTopics[z][w]));
				}
			}

			bw.write("***[[BACKGROUND-TOPIC]]***\n");
			topWords = RankingUtils.getIndexTopElements(k * 2, backgroundTopic);
			for (int j = topWords.size() - 1; j >= 0; j--) {
				int w = topWords.get(j);
				bw.write(String.format("%s,%f\n", tweetVocabulary[w], backgroundTopic[w]));
			}

			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out tweet topic top words to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputEmpiricalGlobalTopicDistribution() {
		int[] nTweetsByTopic = new int[nTopics];
		for (int z = 0; z < nTopics; z++) {
			nTweetsByTopic[z] = 0;
		}
		int nAllTweets = 0;
		for (int e = 0; e < epoches.length; e++) {
			for (int t = 0; t < epoches[e].tweets.length; t++) {
				int z = epoches[e].tweets[t].inferedTopic;
				nTweetsByTopic[z]++;
			}
			nAllTweets += epoches[e].tweets.length;
		}
		try {
			String fileName = outputPath + "/empiricalGlobalTopicDistribution.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			for (int z = 0; z < nTopics; z++) {
				bw.write(String.format("%d,%f\n", z, (double) nTweetsByTopic[z] / nAllTweets));
			}
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out empirical topic distribution!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputTopicDistributionOverTime() {
		int[][] nTweets = new int[epoches.length][nTopics];
		for (int e = 0; e < epoches.length; e++) {
			for (int z = 0; z < nTopics; z++) {
				nTweets[e][z] = 0;
			}
		}
		for (int e = 0; e < epoches.length; e++) {
			for (int t = 0; t < epoches[e].tweets.length; t++) {
				int z = epoches[e].tweets[t].inferedTopic;
				nTweets[e][z]++;
			}
		}

		Calendar cal = Calendar.getInstance();
		SimpleDateFormat formatter = new SimpleDateFormat("MM/dd/yy:HH:mm:ss");

		String[] timeStamps = new String[epoches.length];
		for (int e = 0; e < epoches.length; e++) {
			long time = startTime + e * TIME_STEP_WIDTH;
			cal.setTimeInMillis(time);
			timeStamps[e] = formatter.format(cal.getTime());
		}

		try {
			String fileName = outputPath + "/topicDistributionOverTime.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			bw.write("topic,proportion,timeStep\n");
			for (int e = 0; e < epoches.length; e++) {
				for (int z = 0; z < nTopics; z++) {
					bw.write(String.format("Topic-%d,%d,%s\n", z, nTweets[e][z], timeStamps[e]));
				}
			}
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out topic distribution overtime!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputTweetTopicTopTweetsByPerplexity(int k) {
		int[] tweetPerTopicCount = new int[nTopics];
		int tweetBackgroundTopicCount = 0;
		for (int z = 0; z < nTopics; z++)
			tweetPerTopicCount[z] = 0;
		for (int e = 0; e < epoches.length; e++) {
			for (int t = 0; t < epoches[e].tweets.length; t++) {
				if (epoches[e].tweets[t].inferedTopic >= 0)
					tweetPerTopicCount[epoches[e].tweets[t].inferedTopic]++;
				else
					tweetBackgroundTopicCount++;
			}
		}

		String[][] tweetID = new String[nTopics][];
		int[][] tweetEpochIndex = new int[nTopics][];
		int[][] tweetIndex = new int[nTopics][];

		double[][] perTweetPerplexity = new double[nTopics][];
		for (int z = 0; z < nTopics; z++) {
			tweetID[z] = new String[tweetPerTopicCount[z]];
			perTweetPerplexity[z] = new double[tweetPerTopicCount[z]];
			tweetEpochIndex[z] = new int[tweetPerTopicCount[z]];
			tweetIndex[z] = new int[tweetPerTopicCount[z]];
			tweetPerTopicCount[z] = 0;
		}
		String[] backgroundTweetID = new String[tweetBackgroundTopicCount];
		double[] backgroundTweetPerplexity = new double[tweetBackgroundTopicCount];
		tweetBackgroundTopicCount = 0;

		for (int e = 0; e < epoches.length; e++) {
			for (int t = 0; t < epoches[e].tweets.length; t++) {
				int z = epoches[e].tweets[t].inferedTopic;
				if (z >= 0) {
					tweetID[z][tweetPerTopicCount[z]] = epoches[e].tweets[t].tweetID;
					perTweetPerplexity[z][tweetPerTopicCount[z]] = epoches[e].tweets[t].inferedLikelihood
							/ epoches[e].tweets[t].words.length;
					tweetEpochIndex[z][tweetPerTopicCount[z]] = e;
					tweetIndex[z][tweetPerTopicCount[z]] = t;

					tweetPerTopicCount[z]++;
				} else {
					backgroundTweetID[tweetBackgroundTopicCount] = epoches[e].tweets[t].tweetID;
					backgroundTweetPerplexity[tweetBackgroundTopicCount] = epoches[e].tweets[t].inferedLikelihood;
					tweetBackgroundTopicCount++;
				}

			}
		}

		try {
			String fileName = outputPath + "/tweetTopicTopTweetsByPerplexity.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			List<Integer> topTweets = null;
			HashSet<String> topTweetIds = null;
			for (int z = 0; z < nTopics; z++) {
				topTweetIds = new HashSet<String>();
				bw.write(String.format("***[[TOPIC-%d]]***\n", z));
				topTweets = RankingUtils.getIndexTopElements(k, perTweetPerplexity[z]);
				for (int j = topTweets.size() - 1; j >= 0; j--) {
					int t = topTweets.get(j);
					bw.write(String.format("%s,%f,%s\n", tweetID[z][t], perTweetPerplexity[z][t],
							allRawTweets.get(tweetID[z][t])));
					topTweetIds.add(tweetID[z][t]);
					tweetId2EpochIndex.put(tweetID[z][t], tweetEpochIndex[z][t]);
					tweetId2TweetIndex.put(tweetID[z][t], tweetIndex[z][t]);
				}
				topicTopTweets.put(z, topTweetIds);
			}
			bw.write("***[[BACKGROUND-TOPIC]]***\n");
			topTweets = RankingUtils.getIndexTopElements(k, backgroundTweetPerplexity);
			for (int j = topTweets.size() - 1; j >= 0; j--) {
				int t = topTweets.get(j);
				bw.write(String.format("%s,%f,%s\n", backgroundTweetID[t], backgroundTweetPerplexity[t],
						allRawTweets.get(backgroundTweetID[t])));
			}
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out tweet topic top tweets to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputTweetTopicTopTweetsEntropy(int k) {
		int[] tweetPerTopicCount = new int[nTopics];

		for (int z = 0; z < nTopics; z++)
			tweetPerTopicCount[z] = 0;
		for (int e = 0; e < epoches.length; e++) {
			for (int t = 0; t < epoches[e].tweets.length; t++) {
				if (epoches[e].tweets[t].inferedTopic >= 0)
					tweetPerTopicCount[epoches[e].tweets[t].inferedTopic]++;
			}
		}

		String[][] tweetID = new String[nTopics][];
		int[][] tweetEpochIndex = new int[nTopics][];
		int[][] tweetIndex = new int[nTopics][];

		double[][] perTweetEntropy = new double[nTopics][];
		for (int z = 0; z < nTopics; z++) {
			tweetID[z] = new String[tweetPerTopicCount[z]];
			perTweetEntropy[z] = new double[tweetPerTopicCount[z]];
			tweetEpochIndex[z] = new int[tweetPerTopicCount[z]];
			tweetIndex[z] = new int[tweetPerTopicCount[z]];
			tweetPerTopicCount[z] = 0;
		}

		for (int e = 0; e < epoches.length; e++) {
			for (int t = 0; t < epoches[e].tweets.length; t++) {
				int z = epoches[e].tweets[t].inferedTopic;
				if (z >= 0) {
					tweetID[z][tweetPerTopicCount[z]] = epoches[e].tweets[t].tweetID;
					perTweetEntropy[z][tweetPerTopicCount[z]] = getTweetEntropy(e, t);
					tweetEpochIndex[z][tweetPerTopicCount[z]] = e;
					tweetIndex[z][tweetPerTopicCount[z]] = t;

					tweetPerTopicCount[z]++;
				}
			}
		}

		try {
			String fileName = outputPath + "/tweetTopicTopTweetsByEntropy.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			List<Integer> topTweets = null;
			for (int z = 0; z < nTopics; z++) {
				HashSet<String> topTweetIds = topicTopTweets.get(z);
				bw.write(String.format("***[[TOPIC-%d]]***\n", z));
				topTweets = RankingUtils.getIndexTopElements(k, perTweetEntropy[z]);
				for (int j = topTweets.size() - 1; j >= 0; j--) {
					int t = topTweets.get(j);
					bw.write(String.format("%s,%f,%s\n", tweetID[z][t], perTweetEntropy[z][t],
							allRawTweets.get(tweetID[z][t])));
					tweetId2EpochIndex.put(tweetID[z][t], tweetEpochIndex[z][t]);
					tweetId2TweetIndex.put(tweetID[z][t], tweetIndex[z][t]);
					topTweetIds.add(tweetID[z][t]);
				}
			}
			bw.close();
		} catch (Exception exception) {
			System.out.println("Error in writing out tweet topic top tweets to file!");
			exception.printStackTrace();
			System.exit(0);
		}
	}

	private void outputTweetTopicTopTweetsSumProbUniqueWords(int k) {
		int[] tweetPerTopicCount = new int[nTopics];

		for (int z = 0; z < nTopics; z++)
			tweetPerTopicCount[z] = 0;
		for (int e = 0; e < epoches.length; e++) {
			for (int t = 0; t < epoches[e].tweets.length; t++) {
				if (epoches[e].tweets[t].inferedTopic >= 0)
					tweetPerTopicCount[epoches[e].tweets[t].inferedTopic]++;
			}
		}

		String[][] tweetID = new String[nTopics][];
		int[][] tweetEpochIndex = new int[nTopics][];
		int[][] tweetIndex = new int[nTopics][];
		double[][] perTweetEntropy = new double[nTopics][];
		for (int z = 0; z < nTopics; z++) {
			tweetID[z] = new String[tweetPerTopicCount[z]];
			perTweetEntropy[z] = new double[tweetPerTopicCount[z]];
			tweetEpochIndex[z] = new int[tweetPerTopicCount[z]];
			tweetIndex[z] = new int[tweetPerTopicCount[z]];
			tweetPerTopicCount[z] = 0;
		}

		for (int e = 0; e < epoches.length; e++) {
			for (int t = 0; t < epoches[e].tweets.length; t++) {
				int z = epoches[e].tweets[t].inferedTopic;
				if (z >= 0) {
					tweetID[z][tweetPerTopicCount[z]] = epoches[e].tweets[t].tweetID;
					perTweetEntropy[z][tweetPerTopicCount[z]] = getTweetSumProbUniqueWord(e, t, z);
					tweetEpochIndex[z][tweetPerTopicCount[z]] = e;
					tweetIndex[z][tweetPerTopicCount[z]] = t;

					tweetPerTopicCount[z]++;
				}
			}
		}

		try {
			String fileName = outputPath + "/tweetTopicTopTweetsBySumProbUniqueWords.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			List<Integer> topTweets = null;
			for (int z = 0; z < nTopics; z++) {
				HashSet<String> topTweetIds = topicTopTweets.get(z);
				bw.write(String.format("***[[TOPIC-%d]]***\n", z));
				topTweets = RankingUtils.getIndexTopElements(k, perTweetEntropy[z]);
				for (int j = topTweets.size() - 1; j >= 0; j--) {
					int t = topTweets.get(j);
					bw.write(String.format("%s,%f,%s\n", tweetID[z][t], perTweetEntropy[z][t],
							allRawTweets.get(tweetID[z][t])));
					tweetId2EpochIndex.put(tweetID[z][t], tweetEpochIndex[z][t]);
					tweetId2TweetIndex.put(tweetID[z][t], tweetIndex[z][t]);
					topTweetIds.add(tweetID[z][t]);
				}
			}
			bw.close();
		} catch (Exception exception) {
			System.out.println("Error in writing out tweet topic top tweets to file!");
			exception.printStackTrace();
			System.exit(0);
		}
	}

	private List<String> selectRepresentativeTweets(String[] tweetIds, int k, int z) {
		double[][] weightedSimilarity = new double[tweetIds.length][tweetIds.length];
		for (int i = 0; i < tweetIds.length; i++) {
			int e = tweetId2EpochIndex.get(tweetIds[i]);
			int t = tweetId2TweetIndex.get(tweetIds[i]);
			Tweet tweetA = epoches[e].tweets[t];
			for (int j = i; j < tweetIds.length; j++) {
				e = tweetId2EpochIndex.get(tweetIds[j]);
				t = tweetId2TweetIndex.get(tweetIds[j]);
				Tweet tweetB = epoches[e].tweets[t];
				weightedSimilarity[i][j] = SimilarityUtils.getTweetSimilarity(tweetA, tweetB, tweetTopics[z]);
				weightedSimilarity[j][i] = weightedSimilarity[i][j];
			}
		}
		HashSet<Integer> representativeTweetPositions = new HashSet<Integer>();
		List<String> representativeTweetIds = new ArrayList<String>();
		double[] bestSimilarity = new double[tweetIds.length];
		for (int i = 0; i < tweetIds.length; i++) {
			bestSimilarity[i] = Double.NEGATIVE_INFINITY;
		}
		while (representativeTweetPositions.size() < k) {
			// find max
			double max_value = Double.NEGATIVE_INFINITY;
			int max_position = -1;
			for (int i = 0; i < tweetIds.length; i++) {
				if (representativeTweetPositions.contains(i))
					continue;
				double addedValue = 0;
				for (int j = 0; j < tweetIds.length; j++) {
					if (representativeTweetPositions.contains(j))
						continue;
					if (j == i) {
						continue;
					}
					if (weightedSimilarity[i][j] > bestSimilarity[j]) {
						addedValue += weightedSimilarity[i][j] - bestSimilarity[j];
					}
				}
				if (addedValue > max_value) {
					max_value = addedValue;
					max_position = i;
				}
			}
			// add into selected set
			representativeTweetPositions.add(max_position);
			representativeTweetIds.add(tweetIds[max_position]);
			// update best similarity
			for (int j = 0; j < tweetIds.length; j++) {
				if (representativeTweetPositions.contains(j))
					continue;
				if (weightedSimilarity[max_position][j] > bestSimilarity[j]) {
					bestSimilarity[j] = weightedSimilarity[max_position][j];
				}
			}
		}
		return representativeTweetIds;
	}

	private void outputTweetTopicRepresentativeTweets(int k) {
		try {
			String fileName = outputPath + "/tweetTopicRepresentativeTweets.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			List<String> topTweets = null;
			for (int z = 0; z < nTopics; z++) {
				bw.write(String.format("***[[TOPIC-%d]]***\n", z));
				topTweets = selectRepresentativeTweets(
						(String[]) (topicTopTweets.get(z).toArray(new String[topicTopTweets.get(z).size()])), k, z);
				for (int j = 0; j < topTweets.size(); j++) {
					bw.write(String.format("%s\t%s\n", topTweets.get(j), allRawTweets.get(topTweets.get(j))));
				}
			}
			bw.close();
		} catch (Exception exception) {
			System.out.println("Error in writing out tweet topic top tweets to file!");
			exception.printStackTrace();
			System.exit(0);
		}

	}

	private void outputUserTopicDistribution() {
		try {
			String fileName = outputPath + "/userTopics.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			for (int e = 0; e < epoches.length; e++) {
				bw.write(String.format("%d", epoches[e].epochId));
				for (int z = 0; z < nTopics; z++)
					bw.write(String.format(",%f", epoches[e].topicDistribution[z]));
				bw.write("\n");
			}
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out user topic distributions to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputLikelihood() {
		try {
			String fileName = outputPath + "/likelihood.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			bw.write(String.format("%f", tweetLogLikelidhood));
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out tweets to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void learnModel() {
		readData();
		gibbsSampling();
		inferingModelParameters();
		inferTweetTopic();
		getLikelihoodPerplexity();

		outputTweetTopics();
		outputTweetTopicTopWords(20);

		tweetId2EpochIndex = new HashMap<String, Integer>();
		tweetId2TweetIndex = new HashMap<String, Integer>();
		topicTopTweets = new HashMap<Integer, HashSet<String>>();

		outputTweetTopicTopTweetsByPerplexity(200);
		// outputTweetTopicTopTweetsEntropy(200);
		outputTweetTopicTopTweetsSumProbUniqueWords(200);
		outputTweetTopicRepresentativeTweets(50);

		outputUserTopicDistribution();
		outputEmpiricalGlobalTopicDistribution();
		outputTopicDistributionOverTime();
		outputLikelihood();
		outputCoinBias();
	}

	public static void main(String[] args) {

		new Configure();

		TwitterLDA twitterLDA = new TwitterLDA(
				"C:/Users/Tuan-Anh Hoang/Desktop/attt/travel_ban_graphFilteredTweets.txt",
				"C:/Users/Tuan-Anh Hoang/Desktop/attt/travel_ban", 10);
		twitterLDA.readData();
		// twitterLDA.outputReformattedData();
		twitterLDA.learnModel();
	}

}
